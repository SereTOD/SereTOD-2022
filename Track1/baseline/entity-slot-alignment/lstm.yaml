seed: 42
do_train: True
do_predict: True 

paradigm: token_classification
output_dir: output/Entity-Slot-Alignment

save_path: ../data/lstm
train_file: ../data/train.json
validation_file: ../data/dev.json
test_file: ../data/lstm/test_with_triples.json
language: Chinese
test_exists_labels: False 

truncate_in_batch: True
return_token_type_ids: False

model_type: lstm
model_name_or_path: ../data/wordvec/tencent
vocab_file: ../data/wordvec/tencent
hidden_size: 512
aggregation: max_pooling 
head_scale: 2

num_train_epochs: 20
max_seq_length: 512
dataloader_num_workers: 2

generation_max_length: 128
generation_num_beams: 4
predict_with_generate: True 
ignore_pad_token_for_loss: True 

per_device_train_batch_size: 64
per_device_eval_batch_size: 64
gradient_accumulation_steps: 1
# eval_accumulation_steps: 4
learning_rate: 1.0e-3
weight_decay: 1.0e-5
warmup_ratio: 0.1 
max_grad_norm: 1
optim: adamw_torch

load_best_model_at_end: True
metric_for_best_model: micro_f1 
greater_is_better: True 

logging_strategy: steps
logging_steps: 100
evaluation_strategy: epoch
eval_steps: 500
save_strategy: epoch
save_steps: 500


